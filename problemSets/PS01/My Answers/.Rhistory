}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("stringr"),  pkgTest)
#####################
# Problem 1
#####################
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
qnorm(0.05)
qnorm(0.95)
margin_of_error <- 1.64
standard_error <- sd(y)/sqrt(length(y))
sd(y)/sqrt(length(y))
##confidence interval formula is mean(y-bar) +- 1.96 (se) where se = standard deviation
## divided by the square root of the sample size
CI_higher_bound <- (mean(y) + (1.64 * standard_error))
CI_lower_bound <- (mean(y) - (1.64 * standard_error))
mean(y)
mean <- mean(y)
sd(y)
standard_error <- sd(y)/sqrt(length(y)-1)
sd(y)/sqrt(length(y))
CI_higher_bound <- (mean(y) + (1.64 * standard_error))
CI_lower_bound <- (mean(y) - (1.64 * standard_error))
length(y)
standard_error <- sd(y)/sqrt(24)
sd(y)/sqrt(length(y))
CI_higher_bound <- (mean(y) + (1.64 * standard_error))
CI_lower_bound <- (mean(y) - (1.64 * standard_error))
mean(y)
CI_higher_bound <- (mean(y) + (1.65 * standard_error))
CI_lower_bound <- (mean(y) - (1.65 * standard_error))
conf_int <- 0.9
margin <- qt((1-(1-conf_int)/2), df = length(y) -1)*sd(y)/sqrt(length(y)))
margin <- qt((1-(1-conf_int)/2), df = length(y) -1)*sd(y)/sqrt(length(y))
lower_int = mean(y) - margin
upper_int = mean(y) + margin
t_score <- qt(0.990, df=length(y)-1)
lower_99_t <- mean(y)-(t_score)*(sd(y)/sqrt(y)))
lower_99_t <- mean(y)-(t_score)*(sd(y)/sqrt(y))
lower_99_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+(y)*(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+t_score)*(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+t_score)*(sd(y)/sqrt(length(y))
upper_99_t <- mean(y)+t_score) *(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
# remove objects
rm(list=ls())
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("stringr"),  pkgTest)
#####################
# Problem 1
#####################
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
qnorm(0.05)
qnorm(0.95)
margin_of_error <- 1.64
standard_error <- sd(y)/sqrt(24)
sd(y)/sqrt(length(y))
##confidence interval formula is mean(y-bar) +- 1.96 (se) where se = standard deviation
## divided by the square root of the sample size
t_score <- qt(0.990, df=length(y)-1)
lower_99_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
rm(list=ls())
qnorm(0.10)
qnorm(0.90)
qt(0.10)
t_score <- qt(0.990, df=length(y)-1)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
standard_error <- sd(y)/sqrt(24)
sd(y)/sqrt(length(y))
standard_error <- sd(y)/sqrt(25)
t_score <- qt(0.990, df=length(y)-1)
lower_99_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_99_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
mean(y)
mean <- mean(y)
# remove objects
rm(list())
# remove objects
rm(list()
# remove objects
rm()
# remove objects
rm(list=ls())
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
detachAllPackages()
if (length(new.pkg))
lapply(c("stringr"),  pkgTest)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
standard_error <- sd(y)/sqrt(25)
sd(y)/sqrt(length(y))
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_90_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
mean <- mean(y)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("stringr"),  pkgTest)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
standard_error <- sd(y)/sqrt(25)
sd(y)/sqrt(length(y))
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_90_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
mean <- mean(y)
#####################
# load libraries
# set wd
# clear global .envir
#####################
getwd()
setwd(C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01)
setwd(C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01)
setwd(C://Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01)
setwd(C:\\Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01)
setwd(C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01)
#####################
# load libraries
# set wd
# clear global .envir
#####################
getwd()
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01")
getwd()
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/myanswersAG")
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/MyAnswersAG")
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/My Answers AG")
getwd()
# remove objects
rm(list=ls())
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
detachAllPackages()
install.packages(new.pkg,  dependencies = TRUE)
lapply(c("stringr"),  pkgTest)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
standard_error <- sd(y)/sqrt(25)
sd(y)/sqrt(length(y))
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_90_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
mean <- mean(y)
t90 <- qt ((1 -.95)/2,lower.tail=FALSE)
t90 <- qt ((1 -.90)/2,df=length(y)-1,lower.tail=FALSE)
qqnorm(y)
qqline(y)
t_stat <- (mean(y)-100)/(sd(y)/sqrt(length(y)))
P_value <- pt(abs(t_stat), df = length(y)-1, lower.tail = FALSE)
t_test <- t.test(y, mu = 100, alternative = '')
t_test <- t.test(y, mu = 100, alternative = '')
##Next, I calculate the test statistic
t_stat <- (mean(y)-100)/(sd(y)/sqrt(length(y)))
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
View(expenditure)
plot(Y,X1)
View(expenditure)
plot(Y,X1)
plot(expenditure$Y,expenditure$X1)
plot(expenditure$Y,expenditure$X1)
plot(expenditure$Y,expenditure$X2)
plot(expenditure$Y,expenditure$X3)
View(expenditure)
plot(expenditure$Y,expenditure$Region)
hist(expenditure$Y,expenditure$Region)
ggplot(expenditure$Y,expenditure$Region)
plot(expenditure$Y,expenditure$Region)
boxplot(expenditure$Y,expenditure$Region)
plot(expenditure$Y,expenditure$X1,expenditure$X2,expenditure$X3)
plot(expenditure$Y,expenditure$X1)
abline(lm(expenditure$X!~ expenditure$Y))
abline(lm(expenditure$X ~ expenditure$Y))
abline(lm(expenditure$X1 ~ expenditure$Y))
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
abline(lm(expenditure$X1 ~ expenditure$Y),col='red',lty='dashed')
abline(lm(expenditure$X1 ~ expenditure$Y),col='darkblue',lty='dashed')
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X2)
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X1)
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X2)
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X3)
abline(lm(expenditure$X3 ~ expenditure$Y),col='blue',lty='dashed')
boxplot(expenditure$Y,expenditure$Region)
#####################
# load libraries
# set wd
# clear global .envir
#####################
getwd()
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/My Answers AG")
getwd()
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("stringr"),  pkgTest)
#####################
# Problem 1
#####################
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
sum(y)
length(y)
mean(y)
sd(y)
standard_error <- sd(y)/sqrt(25)
sd(y)/sqrt(length(y))
##confidence interval formula is mean(y-bar) +- 1.96 (se) where se = standard deviation
## divided by the square root of the sample size
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(sd(y)/sqrt(length(y)))
upper_90_t <- mean(y)+(t_score) *(sd(y)/sqrt(length(y)))
mean <- mean(y)
t90 <- qt ((1 -.90)/2,df=length(y)-1,lower.tail=FALSE)
##Answer of Q1^
## Q1 Part 2
##First, I look at the assumptions. The small sample size
## means I use the t distribution and not the normal distribution
##Next, I state my null and alternate hypotheses
## My null hypothesis is that the mean IQ in her school <= 100
## The alternative hypothesis is that the mean IQ > 100
##Next, I calculate the test statistic
t_stat <- (mean(y)-100)/(sd(y)/sqrt(length(y)))
##Next, I calculate the p-value
P_value <- pt(abs(t_stat), df = length(y)-1, lower.tail = FALSE)
##As the p-value is greater than the alpha level of 0.05, we
##do not have enough evidence to reject the null hypothesis
## that the mean in the school is less than/equal to 100
#####################
# Problem 2
#####################
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
##Line of best fit code from https://www.statology.org/line-of-best-fit-in-r/
plot(expenditure$Y,expenditure$X1)
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X2)
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X3)
abline(lm(expenditure$X3 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$X1,expenditure$X2)
abline(lm(expenditure$X1 ~ expenditure$X2),col='blue',lty='dashed')
plot(expenditure$X1,expenditure$X3)
plot(expenditure$X2,expenditure$X3)
plot(expenditure$X3,expenditure$X2)
abline(lm(expenditure$X1 ~ expenditure$X2),col='blue',lty='dashed')
##Line of best fit code from https://www.statology.org/line-of-best-fit-in-r/
plot(expenditure$Y,expenditure$X1)
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
plot(expenditure$Y,expenditure$X2)
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
#####################
# load libraries
# set wd
# clear global .envir
#####################
getwd()
setwd("C:/Users/Lenovo/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/My Answers AG")
getwd()
# remove objects
rm(list=ls())
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("stringr"),  pkgTest)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
length(y) #checking the sample size
mean(y) #central tendency, mean
sd(y) #standard deviation(the square root of the sum of squared deviations divided by n-1)
##the standard error describes how the mean varies from sample to sample
standard_error <- sd(y)/sqrt(25)
##using the code for confidence intervals from Tutorial 2
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(standard_error)
upper_90_t <- mean(y)+(t_score) *(standard_error)
mean <- mean(y)
##Next, I calculate the test statistic
##This formula is from Ch.6 of Agresti and Finlay
test_statistic <- (mean(y)-100)/(sd(y)/sqrt(length(y)))
##Next, I calculate the p-value
##Code from:https://cosmosweb.champlain.edu/people/stevens/webtech/R/Chapter-9-R.pdf
P_value <- pt((test_statistic), df = 24, lower.tail = TRUE)
##The same is calculated using the t.test function from Tutorial 2 to double check
t.test(y, mu = 100, alternative = 'less')
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
##Line of best fit code  from https://www.statology.org/line-of-best-fit-in-r/
##Plot code from Tutorial 2 and 3
plot(expenditure$Y,expenditure$X1,
xlab="Per capita expenditure on shelters/housing assistance in state",
ylab="Per capita personal income in state",
main="The Relationship between expenditure on shelters and per capital personal income")
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
cor(expenditure$Y,expenditure$X1)
text(50, 2500,sprintf("Correlation=%s", round(cor(expenditure$X1,expenditure$Y),4)))
plot(expenditure$Y,expenditure$X2,
xlab="Per capita expenditure on shelters/housing assistance in state",
ylab="”Financially insecure” residents in state per 100,000",
main="The Relationship between expenditure
on shelters and No. of residents that are financially insecure")
abline(lm(expenditure$X2 ~ expenditure$Y),col='blue',lty='dashed')
cor(expenditure$Y,expenditure$X2)
text(50, 450,sprintf("Correlation=%s", round(cor(expenditure$Y,expenditure$X2),4)))
plot(expenditure$Y,expenditure$X3,
xlab="Per capita expenditure on shelters/housing assistance in state",
ylab="No. of people per 1000 residing in urban areas in state",
main="Relationship between expenditure on shelters
and no of people per 1000 residing in urban areas")
abline(lm(expenditure$X3 ~ expenditure$Y),col='blue',lty='dashed')
cor(expenditure$Y,expenditure$X3)
text(50, 800,sprintf("Correlation=%s", round(cor(expenditure$Y,expenditure$X3),4)))
plot(expenditure$X1,expenditure$X2,
xlab="Per Capita personal income in state",
ylab="”Financially insecure” residents in state per 100,000",
main="Relationship between per capital personal income
and ”Financially insecure” residents in state per 100,000")
abline(lm(expenditure$X2 ~ expenditure$X1),col='blue',lty='dashed')
cor(expenditure$X1,expenditure$X2)
text(1200, 450,sprintf("Correlation=%s", round(cor(expenditure$X1,expenditure$X2),4)))
plot(expenditure$X1,expenditure$X3,
xlab="Per Capita personal income in state",
ylab="No. of people per 1000 residing in urban areas in state",
main="Relationship between per capita personal income
and People per 1000 residing in urban areas")
abline(lm(expenditure$X3 ~ expenditure$X1),col='blue',lty='dashed')
cor(expenditure$X1,expenditure$X3)
text(1200, 800,sprintf("Correlation=%s", round(cor(expenditure$X1,expenditure$X3),4)))
plot(expenditure$X3,expenditure$X2,
xlab="No. of people per 1000 residing in urban areas in state",
ylab="”Financially insecure” residents in state per 100,000",
main="Relationship between no. of people residing in urban areas
and ”Financially insecure” residents per 100,000")
abline(lm(expenditure$X2 ~ expenditure$X3),col='blue',lty='dashed')
cor(expenditure$X3,expenditure$X2)
text(380,450,sprintf("Correlation=%s", round(cor(expenditure$X3,expenditure$X2),4)))
##Part2
boxplot(expenditure$Y ~ expenditure$Region,
main="Boxplot of per capita expenditure on shelters
by region",
ylab="Per capita expenditure on shelters",
xlab="Region",
names=c("Northeast","North Central","South","West"))
##In addition to the boxplot, I also calculate the mean of the four regions
##To do this, I used the code from:https://www.statology.org/r-mean-by-group/
aggregate(expenditure$Y, list(expenditure$Region), FUN=mean)
Group_means <- aggregate(expenditure$Y, list(expenditure$Region), FUN=mean)
Group_means
##I took the help of this website:https://r-coder.com/scatter-plot-r/#:~:text=The%20scatterplot%20function%20in%20R,-An%20alternative%20to&text=In%20order%20to%20customize%20the,parameters%20of%20the%20corresponding%20estimates..
##Using the car package, I made a scatterplot
##and by using the ?scatterplot instruction to learn about the customizations possible
##Using this, I made the graph look more legible and clean
## This forum also helped: https://stackoverflow.com/questions/52876568/how-to-use-legend-argument-in-scatterplot-using-car-package-version-3
##I make a scatterplot with Y and X1 and use different colors and shapes to specify the region
library(car)
?scatterplot
scatterplot(expenditure$X1 ~ expenditure$Y|
expenditure$Region,
regLine=TRUE,smooth=FALSE, grid=FALSE,
legend=c(title="Region",coords="topleft"),
main="The relationship between per capita personal income
and per capita expenditure on shelters by region",
xlab="Per capita personal income in state",
ylab="Per capita expenditure on shelters in state")
# remove objects
rm(list=ls())
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("stringr"),  pkgTest)
y <- c(105,69,86,100,82,111,104,110,87,108,87,90,94,113,112,98,
80,97,95,111,114,89,95,126,98)
length(y) #checking the sample size
mean(y) #central tendency, mean
sd(y) #standard deviation(the square root of the sum of squared deviations divided by n-1)
##the standard error describes how the mean varies from sample to sample
standard_error <- sd(y)/sqrt(25)
##using the code for confidence intervals from Tutorial 2
t_score <- qt(0.95, df=length(y)-1)
lower_90_t <- mean(y)-(t_score)*(standard_error)
upper_90_t <- mean(y)+(t_score) *(standard_error)
mean <- mean(y)
##Next, I calculate the test statistic
##This formula is from Ch.6 of Agresti and Finlay
test_statistic <- (mean(y)-100)/(sd(y)/sqrt(length(y)))
##Next, I calculate the p-value
##Code from:https://cosmosweb.champlain.edu/people/stevens/webtech/R/Chapter-9-R.pdf
P_value <- pt((test_statistic), df = 24, lower.tail = TRUE)
##The same is calculated using the t.test function from Tutorial 2 to double check
t.test(y, mu = 100, alternative = 'less')
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
##Line of best fit code  from https://www.statology.org/line-of-best-fit-in-r/
##Plot code from Tutorial 2 and 3
plot(expenditure$Y,expenditure$X1,
xlab="Per capita expenditure on shelters/housing assistance in state",
ylab="Per capita personal income in state",
main="The Relationship between expenditure on shelters and per capital personal income")
abline(lm(expenditure$X1 ~ expenditure$Y),col='blue',lty='dashed')
cor(expenditure$Y,expenditure$X1)
text(50, 2500,sprintf("Correlation=%s", round(cor(expenditure$X1,expenditure$Y),4)))
plot(expenditure$Y,expenditure$X2,
xlab="Per capita expenditure on shelters/housing assistance in state",
ylab="”Financially insecure” residents in state per 100,000",
main="The Relationship between expenditure
on shelters and No. of residents that are financially insecure")
##Next, I calculate the p-value
##Code from:https://cosmosweb.champlain.edu/people/stevens/webtech/R/Chapter-9-R.pdf
P_value <- pt((test_statistic), df = 24, lower.tail = TRUE)
##The same is calculated using the t.test function from Tutorial 2 to double check
t.test(y, mu = 100, alternative = 'less')
